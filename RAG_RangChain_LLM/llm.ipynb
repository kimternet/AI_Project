{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1단계 - LLM 양자화에 필요한 패키지 설치\n",
    "bitsandbytes: Bitsandbytes는 CUDA 사용자 정의 함수, 특히 8비트 최적화 프로그램, 행렬 곱셈(LLM.int8()) 및 양자화 함수에 대한 경량 래퍼\n",
    "PEFT(Parameter-Efficient Fine-Tuning): 모델의 모든 매개변수를 미세 조정하지 않고도 사전 훈련된 PLM(언어 모델)을 다양한 다운스트림 애플리케이션에 효율적으로 적용 가능\n",
    "accelerate: PyTorch 모델을 더 쉽게 여러 컴퓨터나 GPU에서 사용할 수 있게 해주는 도구"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#양자화에 필요한 패키지 설치\n",
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "!pip install -q -U git+https://github.com/huggingface/accelerate.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2단계 - 트랜스포머에서 BitsandBytesConfig를 통해 양자화 매개변수 정의하기\n",
    "load_in_4bit=True: 모델을 4비트 정밀도로 변환하고 로드하도록 지정\n",
    "bnb_4bit_use_double_quant=True: 메모리 효율을 높이기 위해 중첩 양자화를 사용하여 추론 및 학습\n",
    "bnd_4bit_quant_type=\"nf4\": 4비트 통합에는 2가지 양자화 유형인 FP4와 NF4가 제공됨. NF4 dtype은 Normal Float 4를 나타내며 QLoRA 백서에 소개되어 있습니다. 기본적으로 FP4 양자화 사용\n",
    "bnb_4bit_compute_dype=torch.bfloat16: 계산 중 사용할 dtype을 변경하는 데 사용되는 계산 dtype. 기본적으로 계산 dtype은 float32로 설정되어 있지만 계산 속도를 높이기 위해 bf16으로 설정 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/LLM/env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3단계 - 경량화 모델 로드하기\n",
    "이제 모델 ID를 지정한 다음 이전에 정의한 양자화 구성으로 로드합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 2/2 [10:04<00:00, 302.08s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.66s/it]\n"
     ]
    }
   ],
   "source": [
    "model_id = \"kyujinpy/Ko-PlatYi-6B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4단계 - 잘 실행되는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"은행의 기준 금리에 대해서 설명해줘\"}\n",
    "]\n",
    "\n",
    "# Define a simple chat template\n",
    "chat_template = \"{% for message in messages %}{% if message.role == 'user' %}### User:\\n{{ message.content }}\\n{% elif message.role == 'assistant' %}### Assistant:\\n{{ message.content }}\\n{% endif %}{% endfor %}\"\n",
    "\n",
    "# Apply the chat template\n",
    "encodeds = tokenizer.apply_chat_template(messages, chat_template=chat_template, return_tensors=\"pt\")\n",
    "\n",
    "model_inputs = encodeds.to(device)\n",
    "\n",
    "\n",
    "generated_ids = model.generate(model_inputs, max_new_tokens=1000, do_sample=True)\n",
    "decoded = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5단계- RAG 시스템 결합하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install시 utf-8, ansi 관련 오류날 경우 필요한 코드\n",
    "import locale\n",
    "def getpreferredencoding(do_setlocale = True):\n",
    "    return \"UTF-8\"\n",
    "locale.getpreferredencoding = getpreferredencoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/LLM/env/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:151: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 0.3. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFacePipeline`.\n",
      "  warn_deprecated(\n",
      "/home/LLM/env/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:151: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from transformers import pipeline\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "text_generation_pipeline = pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    temperature=0.2,\n",
    "    return_full_text=True,\n",
    "    max_new_tokens=300,\n",
    ")\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "### [INST]\n",
    "Instruction: Answer the question based on your knowledge.\n",
    "Here is context to help:\n",
    "\n",
    "{context}\n",
    "\n",
    "### QUESTION:\n",
    "{question}\n",
    "\n",
    "[/INST]\n",
    " \"\"\"\n",
    "\n",
    "koplatyi_llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
    "\n",
    "# Create prompt from prompt template\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=prompt_template,\n",
    ")\n",
    "\n",
    "# Create llm chain\n",
    "llm_chain = LLMChain(llm=koplatyi_llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.schema.runnable import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"/home/LLM/intel.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "texts = text_splitter.split_documents(pages)\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = \"jhgan/ko-sbert-nli\"\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "hf = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "db = FAISS.from_documents(texts, hf)\n",
    "retriever = db.as_retriever(\n",
    "                            search_type=\"similarity\",\n",
    "                            search_kwargs={'k': 3}\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    " {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | llm_chain\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "주어진 근거: 4 월 10 일 - 공강\n",
      "4 월 11 일 - Pandas 소개, Pandas 코드 실습\n",
      "4 월 12 일 - Matplotlib, Scikit-learn 소개, Matplotlib, Scikit-learn 코드 실습\n",
      "4 월 15 일 - 데이터  가져오기  및 처리, Data Import and Processing 실습, Seq_Coding 실습\n",
      "4 월 16 일 -  Basic Data Processing and Visualization 실습, Handling erroneous and \n",
      "missing data 실습, 팀 프로젝트  진행\n",
      "4 월 17 일 - 통계 데이터  연습을  위한 AI, Machine Learning Techniques 실습\n",
      "4 월 18 일 - Regression 실습 \n",
      "4 월 19 일 - Class 와 pygae 을 이용한  간단한  게임제작 , 캡스톤  팀 프로젝트 , Rossmann \n",
      "매장 데이터를  이용한  Sales 예측 프로젝트 / 출처: /home/LLM/intel.pdf - 1 \n",
      "\n",
      "\n",
      "주어진 근거: 4 월 2 일 - 새로운  기술 - AI, 쉬운 AI, AI 안에 무엇이  있는가 ?, AI 받아들이기 , 포용적  AI, AI \n",
      "목표설정 \n",
      "4 월 3 일 - AI 프로젝트  사이클  소개, 4W 소개, 데이터  소스, 데이터  탐색, 모델링 , 평가, \n",
      "배포, 지도학습 , 비지도학습 , No 코드 소프트웨어인  Orange 를 이요한  AI 작업(Employee \n",
      "Attrition Prediction, Predictive Maintenance) \n",
      "4 월 4 일 - No 코드 소프트웨어인  Orange 를 이용한  AI 작업 : Insurance Fraud \n",
      "Detection,No 코드 소프트웨어인  Orange 를 이용한  AI 작업 : Quality Assurance System, \n",
      "No 코드 소프트웨어인  Orange 를 이용한  AI 작업 : Viral Post Prediction, No 코드 / 출처: /home/LLM/intel.pdf - 0 \n",
      "\n",
      "\n",
      "주어진 근거: 소프트웨어인  Orange 를 이용한  AI 작업 : Image Classification, Python 코딩 기초1\n",
      "4 월 5 일 - Python 코딩 기초2\n",
      "4 월 8 일 - Python 코딩 기초3, Python 코딩 실습\n",
      "4 월 9 일 - Numpy 소개, Numpy 코드 실습 / 출처: /home/LLM/intel.pdf - 0 \n",
      "\n",
      "\n",
      "\n",
      "답변: \n",
      "### [INST]\n",
      "Instruction: Answer the question based on your knowledge.\n",
      "Here is context to help:\n",
      "\n",
      "[Document(metadata={'source': '/home/LLM/intel.pdf', 'page': 1}, page_content='4 월 10 일 - 공강\\n4 월 11 일 - Pandas 소개, Pandas 코드 실습\\n4 월 12 일 - Matplotlib, Scikit-learn 소개, Matplotlib, Scikit-learn 코드 실습\\n4 월 15 일 - 데이터  가져오기  및 처리, Data Import and Processing 실습, Seq_Coding 실습\\n4 월 16 일 -  Basic Data Processing and Visualization 실습, Handling erroneous and \\nmissing data 실습, 팀 프로젝트  진행\\n4 월 17 일 - 통계 데이터  연습을  위한 AI, Machine Learning Techniques 실습\\n4 월 18 일 - Regression 실습 \\n4 월 19 일 - Class 와 pygae 을 이용한  간단한  게임제작 , 캡스톤  팀 프로젝트 , Rossmann \\n매장 데이터를  이용한  Sales 예측 프로젝트'), Document(metadata={'source': '/home/LLM/intel.pdf', 'page': 0}, page_content='4 월 2 일 - 새로운  기술 - AI, 쉬운 AI, AI 안에 무엇이  있는가 ?, AI 받아들이기 , 포용적  AI, AI \\n목표설정 \\n4 월 3 일 - AI 프로젝트  사이클  소개, 4W 소개, 데이터  소스, 데이터  탐색, 모델링 , 평가, \\n배포, 지도학습 , 비지도학습 , No 코드 소프트웨어인  Orange 를 이요한  AI 작업(Employee \\nAttrition Prediction, Predictive Maintenance) \\n4 월 4 일 - No 코드 소프트웨어인  Orange 를 이용한  AI 작업 : Insurance Fraud \\nDetection,No 코드 소프트웨어인  Orange 를 이용한  AI 작업 : Quality Assurance System, \\nNo 코드 소프트웨어인  Orange 를 이용한  AI 작업 : Viral Post Prediction, No 코드'), Document(metadata={'source': '/home/LLM/intel.pdf', 'page': 0}, page_content='소프트웨어인  Orange 를 이용한  AI 작업 : Image Classification, Python 코딩 기초1\\n4 월 5 일 - Python 코딩 기초2\\n4 월 8 일 - Python 코딩 기초3, Python 코딩 실습\\n4 월 9 일 - Numpy 소개, Numpy 코드 실습')]\n",
      "\n",
      "### QUESTION:\n",
      "4월에는 무엇을 배웠지?\n",
      "\n",
      "[/INST]\n",
      " 4월 10일 - 공강\n",
      "4월 11일 - Pandas 소개, Pandas 코드 실습\n",
      "4월 12일 - Matplotlib, Scikit-learn 소개, Matplotlib, Scikit-learn 코드 실습\n",
      "4월 15일 - 데이터 가져오기 및 처리, Data Import and Processing 실습, Seq_Coding 실습\n",
      "4월 16일 - Basic Data Processing and Visualization 실습, Handling erroneous and missing data 실습, 팀 프로젝트 진행\n",
      "4월 17일 - 통계 데이터 연습을 위한 AI, Machine Learning Techniques 실습\n",
      "4월 18일 - Regression 실습\n",
      "4월 19일 - Class 와 pygae 을 이용한 간단한 게임 제작, 캡스톤 팀 프로젝트, Rossmann 매장 데이터를 이용한 Sales 예측 프로젝트\n"
     ]
    }
   ],
   "source": [
    "result = rag_chain.invoke(\"4월에는 무엇을 배웠지?\")\n",
    "\n",
    "for i in result['context']:\n",
    "    print(f\"주어진 근거: {i.page_content} / 출처: {i.metadata['source']} - {i.metadata['page']} \\n\\n\")\n",
    "\n",
    "print(f\"\\n답변: {result['text']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
